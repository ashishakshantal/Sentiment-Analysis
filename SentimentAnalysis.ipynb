{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import nltk \n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/sagar/cpee/sentiment analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "f1 = open(\"positive.txt\",\"r\",encoding='latin-1')   # \"r\" is for reading\n",
    "short_pos = f1.read() \n",
    "f2 = open(\"negative.txt\",\"r\",encoding='latin-1')\n",
    "short_neg = f2.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_pos = short_pos.lower()\n",
    "short_neg = short_neg.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a long string into list of string with new line as delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "simplistic , silly and tedious . \n"
     ]
    }
   ],
   "source": [
    "posidocuments = short_pos.split('\\n')\n",
    "print(posidocuments[0])\n",
    "negadocuments = short_neg.split('\\n')\n",
    "print(negadocuments[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing data size for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posidocuments = posidocuments[:1000]\n",
    "negadocuments = negadocuments[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making common document database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for p in posidocuments:\n",
    "    documents.append((p, \"pos\"))\n",
    "for n in negadocuments:\n",
    "    documents.append((n, \"neg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Tokenizer and Stop-Words configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import RegexpTokenizer as regextoken\n",
    "tokenizer = regextoken(r'\\w+')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging - Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('rock', 'NN'), ('is', 'VBZ'), ('destined', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('the', 'DT'), ('21st', 'JJ'), ('century', 'NN'), ('s', 'VBD'), ('new', 'JJ'), ('conan', 'NN'), ('and', 'CC'), ('that', 'IN'), ('he', 'PRP'), ('s', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('make', 'VB'), ('a', 'DT'), ('splash', 'NN'), ('even', 'RB'), ('greater', 'JJR'), ('than', 'IN'), ('arnold', 'RB'), ('schwarzenegger', 'JJ'), ('jean', 'JJ'), ('claud', 'NN'), ('van', 'NN'), ('damme', 'NN'), ('or', 'CC'), ('steven', 'JJ'), ('segal', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "words = tokenizer.tokenize(posidocuments[0])\n",
    "print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allowed_word_types = [\"JJ\"]\n",
    "all_words = []\n",
    "for doc, label in documents:\n",
    "    # tokens with one or more alpha-numerics (special characters will not be included)\n",
    "    words = tokenizer.tokenize(doc)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    for word in tagged_words:\n",
    "        if word[1] in allowed_word_types:\n",
    "            # extracting the adjectives\n",
    "            all_words.append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 Documents\n",
      "4579 Adjectives\n"
     ]
    }
   ],
   "source": [
    "print(len(documents),'Documents')\n",
    "print(len(all_words),'Adjectives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking structure of documents variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"it's a bittersweet and lyrical mix of elements . \", 'pos'),\n",
       " ('subversive , meditative , clinical and poetic , the piano teacher is a daring work of genius . ',\n",
       "  'pos'),\n",
       " ('simplistic , silly and tedious . ', 'neg'),\n",
       " (\"it's so laddish and juvenile , only teenage boys could possibly find it funny . \",\n",
       "  'neg')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[998:1002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Frequency Distribution of word-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 92),\n",
       " ('good', 84),\n",
       " ('bad', 51),\n",
       " ('little', 50),\n",
       " ('much', 48),\n",
       " ('new', 41),\n",
       " ('other', 41),\n",
       " ('funny', 38),\n",
       " ('t', 37),\n",
       " ('old', 36)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating list of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1751\n",
      "['21st', 'new', 'schwarzenegger', 'jean', 'steven', 'elaborate', 'huge', 'describe', 'co', 'middle']\n"
     ]
    }
   ],
   "source": [
    "word_features = list(all_words.keys())\n",
    "## Alternative method using set()\n",
    "# print(len(set(all_words)))\n",
    "print(len(word_features))\n",
    "print(word_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords and creating Feature-Set from Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 Documents converted into Feature-Set\n"
     ]
    }
   ],
   "source": [
    "def find_features(document):\n",
    "    document_tokens = tokenizer.tokenize(document)\n",
    "    \n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in document_tokens and w not in stopwords)     \n",
    "    return features\n",
    "\n",
    "featuresets  = [(find_features(rev), category) for (rev, category) in documents]\n",
    "print(len(featuresets),'Documents converted into Feature-Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the documents and creating Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(featuresets)\n",
    "training_set = featuresets[:1800] \n",
    "testing_set =  featuresets[1800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 87.8888888888889\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "train_acc = nltk.classify.accuracy(classifier, training_set)*100\n",
    "print('Training Accuracy',train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Informative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  stupid = True              neg : pos    =      6.8 : 1.0\n",
      "                personal = True              pos : neg    =      5.8 : 1.0\n",
      "                animated = True              pos : neg    =      5.8 : 1.0\n",
      "           psychological = True              pos : neg    =      5.1 : 1.0\n",
      "               adventure = True              pos : neg    =      5.1 : 1.0\n",
      "                     bad = True              neg : pos    =      5.0 : 1.0\n",
      "               contrived = True              neg : pos    =      4.9 : 1.0\n",
      "                   crazy = True              neg : pos    =      4.9 : 1.0\n",
      "                  strong = True              pos : neg    =      4.5 : 1.0\n",
      "                    nice = True              pos : neg    =      4.5 : 1.0\n",
      "                  deeply = True              pos : neg    =      4.5 : 1.0\n",
      "                   aware = True              pos : neg    =      4.5 : 1.0\n",
      "                possible = True              pos : neg    =      4.5 : 1.0\n",
      "                    cute = True              pos : neg    =      4.5 : 1.0\n",
      "                    warm = True              pos : neg    =      4.5 : 1.0\n",
      "             provocative = True              pos : neg    =      4.5 : 1.0\n",
      "                 genuine = True              pos : neg    =      4.5 : 1.0\n",
      "               wonderful = True              pos : neg    =      4.5 : 1.0\n",
      "               effective = True              pos : neg    =      4.5 : 1.0\n",
      "             fascinating = True              pos : neg    =      4.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59 44]\n",
      " [29 68]]\n",
      "Testing Accuracy: 63.5\n"
     ]
    }
   ],
   "source": [
    "predicted=classifier.classify_many([x[0] for x in testing_set])\n",
    "conf_matrix = confusion_matrix(predicted, [x[1] for x in testing_set])\n",
    "print(conf_matrix) \n",
    "\n",
    "print(\"Testing Accuracy:\", (nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
